% This file was created with Citavi 6.10.0.0

@proceedings{.,
 title = {Submissions to the 2019 Kidney Tumor Segmentation Challenge: KiTS19},
 publisher = {{University of Minnesota Libraries Publishing}},
 doi = {10.24926/548719}
}


@misc{.08.05.2022,
 year = {08.05.2022},
 title = {Image Classification vs Semantic Segmentation vs Instance Segmentation | by Nirmala Murali | Medium},
 url = {https://nirmalamurali.medium.com/image-classification-vs-semantic-segmentation-vs-instance-segmentation-625c33a08d50},
 urldate = {08.05.2022}
}


@book{.2005,
 year = {2005},
 title = {Radiopaedia.org},
 publisher = {Radiopaedia.org}
}


@proceedings{.2016,
 year = {2016},
 title = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 isbn = {1063-6919}
}


@proceedings{.2021,
 year = {2021},
 title = {2021 IEEE Symposium Series on Computational Intelligence (SSCI): Virtual Conference 5-7 December 2021},
 address = {Piscataway, New Jersey},
 publisher = {IEEE},
 isbn = {978-1-7281-9048-8}
}


@misc{.23Aug22,
 abstract = {{\textquotedbl}{\textgreater}{\textless}meta name=},
 year = {23-Aug-22},
 title = {QDOSE | Personalised Dosimetry in Molecular Radiotherapy},
 url = {https://www.quantitativedose.com/#features},
 urldate = {23-Aug-22}
}


@misc{.25Aug22,
 year = {25-Aug-22},
 title = {Getting Started with Mask R-CNN for Instance Segmentation - MATLAB {\&} Simulink - MathWorks Deutschland},
 url = {https://de.mathworks.com/help/vision/ug/getting-started-with-mask-r-cnn-for-instance-segmentation.html},
 urldate = {25-Aug-22}
}


@misc{.25Nov22,
 year = {25-Nov-22},
 title = {EUR-Lex - 32013L0059 - EN - EUR-Lex},
 url = {https://eur-lex.europa.eu/eli/dir/2013/59/oj},
 urldate = {25-Nov-22}
}


@misc{.25Nov22b,
 year = {25-Nov-22},
 title = {ABX-CRO advanced pharmaceutical services Forschungsgesellschaft mbH |},
 url = {https://www.abx-cro.com/},
 urldate = {25-Nov-22}
}


@proceedings{.op.2017,
 year = {op. 2017},
 title = {ICCV 2017},
 address = {Los Alamitos and Washington and Tokyo},
 publisher = {CPS and {IEEE Computer Society}},
 isbn = {978-1-5386-1032-9},
 series = {Proceedings (IEEE International Conference on Computer Vision. Online)}
}


@article{Ahn.2019,
 abstract = {BACKGROUND

Accurate and standardized descriptions of organs at risk (OARs) are essential in radiation therapy for treatment planning and evaluation. Traditionally, physicians have contoured patient images manually, which, is time-consuming and subject to inter-observer variability. This study aims to a) investigate whether customized, deep-learning-based auto-segmentation could overcome the limitations of manual contouring and b) compare its performance against a typical, atlas-based auto-segmentation method organ structures in liver cancer.

METHODS

On-contrast computer tomography image sets of 70 liver cancer patients were used, and four OARs (heart, liver, kidney, and stomach) were manually delineated by three experienced physicians as reference structures. Atlas and deep learning auto-segmentations were respectively performed with MIM Maestro 6.5 (MIM Software Inc., Cleveland, OH) and, with a deep convolution neural network (DCNN). The Hausdorff distance (HD) and, dice similarity coefficient (DSC), volume overlap error (VOE), and relative volume difference (RVD) were used to quantitatively evaluate the four different methods in the case of the reference set of the four OAR structures.

RESULTS

The atlas-based method yielded the following average DSC and standard deviation values (SD) for the heart, liver, right kidney, left kidney, and stomach: 0.92 $\pm$ 0.04 (DSC $\pm$ SD), 0.93 $\pm$ 0.02, 0.86 $\pm$ 0.07, 0.85 $\pm$ 0.11, and 0.60 $\pm$ 0.13 respectively. The deep-learning-based method yielded corresponding values for the OARs of 0.94 $\pm$ 0.01, 0.93 $\pm$ 0.01, 0.88 $\pm$ 0.03, 0.86 $\pm$ 0.03, and 0.73 $\pm$ 0.09. The segmentation results show that the deep learning framework is superior to the atlas-based framwork except in the case of the liver. Specifically, in the case of the stomach, the DSC, VOE, and RVD showed a maximum difference of 21.67, 25.11, 28.80{\%} respectively.

CONCLUSIONS

In this study, we demonstrated that a deep learning framework could be used more effectively and efficiently compared to atlas-based auto-segmentation for most OARs in human liver cancer. Extended use of the deep-learning-based framework is anticipated for auto-segmentations of other body sites.},
 author = {Ahn, Sang Hee and Yeo, Adam Unjin and Kim, Kwang Hyeon and Kim, Chankyu and Goh, Youngmoon and Cho, Shinhaeng and Lee, Se Byeong and Lim, Young Kyung and Kim, Haksoo and Shin, Dongho and Kim, Taeyoon and Kim, Tae Hyun and Youn, Sang Hee and Oh, Eun Sang and Jeong, Jong Hwi},
 year = {2019},
 title = {Comparative clinical evaluation of atlas and deep-learning-based auto-segmentation of organ structures in liver cancer},
 url = {https://ro-journal.biomedcentral.com/articles/10.1186/s13014-019-1392-z},
 pages = {213},
 volume = {14},
 number = {1},
 issn = {1748-717X},
 journal = {Radiation Oncology},
 doi = {10.1186/s13014-019-1392-z},
 file = {Ahn, Yeo et al. 2019 - Comparative clinical evaluation of atlas:Attachments/Ahn, Yeo et al. 2019 - Comparative clinical evaluation of atlas.pdf:application/pdf}
}


@article{Aljabri.2022,
 abstract = {Deep learning (DL) algorithms have rapidly become a robust tool for analyzing medical images. They have been used extensively for medical image segmentation as the first and significant components of the diagnosis and treatment pipeline. Medical image segmentation is efficiently addressed by many types of deep neural networks, such as convolutional neural networks, fully convolutional network recurrent networks, adversarial networks, and U-shaped networks. This paper reviews the major DL models and applications pertinent to medical image segmentation and summarizes over 150 contributions to the field. Brief overviews of articles are provided by application area: anatomical structures such as organs, bones, and vessels, and abnormalities such as lesions and calcification. Moreover, we discuss current challenges and suggest directions for future research.},
 author = {Aljabri, Manar and AlGhamdi, Manal},
 year = {2022},
 title = {A review on the use of deep learning for medical images segmentation},
 url = {https://www.sciencedirect.com/science/article/pii/S0925231222009420},
 pages = {311--335},
 volume = {506},
 issn = {0925-2312},
 journal = {Neurocomputing},
 doi = {10.1016/j.neucom.2022.07.070}
}


@article{Altini.2022,
 abstract = {Deep Learning approaches for automatic segmentation of organs from CT scans and MRI are providing promising results, leading towards a revolution in t$\ldots$},
 author = {Altini, Nicola and Prencipe, Berardino and Cascarano, Giacomo Donato and Brunetti, Antonio and Brunetti, Gioacchino and Triggiani, Vito and Carnimeo, Leonarda and Marino, Francescomaria and Guerriero, Andrea and Villani, Laura and Scardapane, Arnaldo and Bevilacqua, Vitoantonio},
 year = {2022},
 title = {Liver, kidney and spleen segmentation from CT scans and MRI with deep learning: A survey},
 url = {https://www-1sciencedirect-1com-1000340sy04a4.han.technikum-wien.at/science/article/pii/S0925231222003149#b0075},
 pages = {30--53},
 volume = {490},
 issn = {0925-2312},
 journal = {Neurocomputing},
 doi = {10.1016/j.neucom.2021.08.157}
}


@misc{Brainder..2012,
 abstract = {(This article is about the nifti-1 file format. For an overview of how the nifti-2 differs from the nifti-1, see this one.) The Neuroimaging Informatics Technology Initiative (nifti) file format wa$\ldots$},
 author = {Brainder.},
 year = {2012},
 title = {The NIFTI file format},
 url = {https://brainder.org/2012/09/23/the-nifti-file-format/},
 urldate = {25-Aug-22}
}


@article{Cai.2020,
 abstract = {Big medical data mainly include electronic health record data, medical image data, gene information data, etc. Among them, medical image data account for the vast majority of medical data at this stage. How to apply big medical data to clinical practice? This is an issue of great concern to medical and computer researchers, and intelligent imaging and deep learning provide a good answer. This review introduces the application of intelligent imaging and deep learning in the field of big data analysis and early diagnosis of diseases, combining the latest research progress of big data analysis of medical images and the work of our team in the field of big data analysis of medical imagec, especially the classification and segmentation of medical images.},
 author = {Cai, Lei and Gao, Jingyang and {Di Zhao}},
 year = {2020},
 title = {A review of the application of deep learning in medical image classification and segmentation},
 url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7327346/#r3},
 pages = {713},
 volume = {8},
 number = {11},
 journal = {Annals of Translational Medicine},
 doi = {10.21037/atm.2020.02.44},
 file = {Cai, Gao et al. 2020 - A review of the application:Attachments/Cai, Gao et al. 2020 - A review of the application.pdf:application/pdf}
}


@inproceedings{Chen.,
 author = {Chen, Cong and Ma, Longfei and Jia, Yan and Zuo, Panli},
 title = {Kidney and Tumor Segmentation Using Modified 3D Mask RCNN},
 publisher = {{University of Minnesota Libraries Publishing}},
 booktitle = {Submissions to the 2019 Kidney Tumor Segmentation Challenge: KiTS19},
 doi = {10.24926/548719.061}
}


@article{DellaGala.2021,
 abstract = {INTRODUCTION

Targeted Radionuclide Therapy (TRT) is a branch of cancer medicine dealing with the therapeutic use of radioisotopes associated with biological vectors accumulating in the tumors/targets, indicated as Molecular Radiotherapy (MRT), or directly injected into the arteries that supply blood to liver tumour vasculature, indicated as Selective RT (SRT). The aim of this work is to offer a panoramic view on the increasing number of commercially-available TRT treatment planning systems (TPSs).

MATERIALS AND METHODS

A questionnaire was sent to manufacturers' representatives. Academic software were not considered. Questions were grouped as follows: general information, clinical workflow, calibration procedure, image processing/reconstruction, image registration and segmentation tools, time-activity curve (TAC) fitting and absorbed dose calculation.

RESULTS

All software reported have CE-marking. TPSs were divided between SRT-dedicated software [4] and MRT [5] dosimetry software. In SRT, since no kinetic process is involved, absorbed dose calculation does not require TAC fitting, and image registration is not fully developed in all TPS. All software requires a radionuclide-specific calibration. In SRT, a relative image calibration can be obtained by scaling the counts to a known activity. Automated VOI contouring and rigid/deformable propagation between different acquisitions time-points is implemented in most TPSs, although DICOM export is rare. Different TAC fits are available depending on the number of time-points. Voxel S-value and Local deposition methods are the most frequent dosimetric approaches; dose-voxel kernel convolution and semi-Monte Carlo method are also available.

CONCLUSIONS

Available TPSs allows performing personalized dosimetry in clinical practice. Individual variations in methodology/algorithms must be considered in the standardisation/harmonization processes.},
 author = {{Della Gala}, Giuseppe and Bardi{\`e}s, Manuel and Tipping, Jill and Strigari, Lidia},
 year = {2021},
 title = {Overview of commercial treatment planning systems for targeted radionuclide therapy},
 pages = {52--61},
 volume = {92},
 journal = {Physica medica : PM : an international journal devoted to the applications of physics to medicine and biology : official journal of the Italian Association of Biomedical Physics (AIFB)},
 doi = {10.1016/j.ejmp.2021.11.001}
}


@article{Dice.1945,
 author = {Dice, Lee R.},
 year = {1945},
 title = {Measures of the Amount of Ecologic Association Between Species},
 pages = {297--302},
 volume = {26},
 number = {3},
 issn = {00129658},
 journal = {Ecology},
 doi = {10.2307/1932409}
}


@proceedings{Drukker.2022,
 year = {2022},
 title = {Medical Imaging 2022: Computer-Aided Diagnosis},
 publisher = {{SPIE / International Society for Optical Engineering}},
 isbn = {9781510649415},
 editor = {Drukker, Karen}
}


@misc{Girshick.11Nov13,
 abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012---achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
 author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
 date = {11-Nov-13},
 title = {Rich feature hierarchies for accurate object detection and semantic  segmentation},
 url = {https://arxiv.org/pdf/1311.2524},
 file = {Girshick, Donahue et al. 11-Nov-13 - Rich feature hierarchies for accurate:Attachments/Girshick, Donahue et al. 11-Nov-13 - Rich feature hierarchies for accurate.pdf:application/pdf}
}


@misc{Girshick.30Apr15,
 abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
 author = {Girshick, Ross},
 date = {30-Apr-15},
 title = {Fast R-CNN},
 url = {https://arxiv.org/pdf/1504.08083},
 file = {Girshick 30-Apr-15 - Fast R-CNN:Attachments/Girshick 30-Apr-15 - Fast R-CNN.pdf:application/pdf}
}


@misc{GitHub.25Aug22,
 abstract = {Mask-RCNN training and prediction in MATLAB for Instance Segmentation - GitHub - matlab-deep-learning/mask-rcnn: Mask-RCNN training and prediction in MATLAB for Instance Segmentation},
 author = {GitHub},
 year = {25-Aug-22},
 title = {GitHub - matlab-deep-learning/mask-rcnn: Mask-RCNN training and prediction in MATLAB for Instance Segmentation},
 url = {https://github.com/matlab-deep-learning/mask-rcnn},
 urldate = {25-Aug-22}
}


@inproceedings{Goyal.2022,
 author = {Goyal, Manu and Guo, Junyu and Hinojosa, Lauren and Hulsey, Keith and Pedrosa, Ivan},
 title = {Automated kidney segmentation by mask R-CNN in T2-weighted magnetic resonance imaging},
 url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12033/2612449/Automated-kidney-segmentation-by-mask-R-CNN-in-T2-weighted/10.1117/12.2612449.full},
 pages = {134},
 publisher = {{SPIE / International Society for Optical Engineering}},
 isbn = {9781510649415},
 editor = {Drukker, Karen},
 booktitle = {Medical Imaging 2022: Computer-Aided Diagnosis},
 year = {2022},
 doi = {10.1117/12.2612449},
 file = {Goyal, Guo et al. 2 20 2022 - 3 28 2022 - Automated kidney segmentation by mask:Attachments/Goyal, Guo et al. 2 20 2022 - 3 28 2022 - Automated kidney segmentation by mask.pdf:application/pdf}
}


@inproceedings{He.op.2017,
 author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
 title = {Mask R-CNN},
 pages = {2980--2988},
 publisher = {CPS and {IEEE Computer Society}},
 isbn = {978-1-5386-1032-9},
 series = {Proceedings (IEEE International Conference on Computer Vision. Online)},
 booktitle = {ICCV 2017},
 year = {op. 2017},
 address = {Los Alamitos and Washington and Tokyo},
 doi = {10.1109/ICCV.2017.322}
}


@article{Heimann.2009,
 author = {Heimann, Tobias and {van Ginneken}, Bram and Styner, Martin A. and Arzhaeva, Yulia and Aurich, Volker and Bauer, Christian and Beck, Andreas and Becker, Christoph and Beichel, Reinhard and Bekes, Gy{\"O}rgy and Bello, Fernando and Binnig, Gerd and Bischof, Horst and Bornik, Alexander and Cashman, Peter M. M. and Chi, Ying and Cordova, Andr{\'E}s and Dawant, Benoit M. and Fidrich, M{\'A}rta and Furst, Jacob D. and Furukawa, Daisuke and Grenacher, Lars and Hornegger, Joachim and Kainm{\"U}ller, Dagmar and Kitney, Richard I. and Kobatake, Hidefumi and Lamecker, Hans and Lange, Thomas and Lee, Jeongjin and Lennon, Brian and Li, Rui and Li, Senhu and Meinzer, Hans-Peter and Nemeth, G{\'A}bor and Raicu, Daniela S. and Rau, Anne-Mareike and {van Rikxoort}, Eva M. and Rousson, Mika{\"E}l and Rusko, L{\'A}szl{\'O} and Saddi, Kinda A. and Schmidt, G{\"U}nter and Seghers, Dieter and Shimizu, Akinobu and Slagmolen, Pieter and Sorantin, Erich and Soza, Grzegorz and Susomboon, Ruchaneewan and Waite, Jonathan M. and Wimmer, Andreas and Wolf, Ivo},
 year = {2009},
 title = {Comparison and Evaluation of Methods for Liver Segmentation From CT Datasets},
 pages = {1251--1265},
 volume = {28},
 number = {8},
 journal = {IEEE Transactions on Medical Imaging},
 doi = {10.1109/TMI.2009.2013851}
}


@article{Howe.1999,
 abstract = {Robotic technology is enhancing surgery through improved precision, stability, and dexterity. In image-guided procedures, robots use magnetic resonance and computed tomography image data to guide instruments to the treatment site. This requires new algorithms and user interfaces for planning procedures; it also requires sensors for registering the patient's anatomy with the preoperative image data. Minimally invasive procedures use remotely controlled robots that allow the surgeon to work inside the patient's body without making large incisions. Specialized mechanical designs and sensing technologies are needed to maximize dexterity under these access constraints. Robots have applications in many surgical specialties. In neurosurgery, image-guided robots can biopsy brain lesions with minimal damage to adjacent tissue. In orthopedic surgery, robots are routinely used to shape the femur to precisely fit prosthetic hip joint replacements. Robotic systems are also under development for closed-chest heart bypass, for microsurgical procedures in ophthalmology, and for surgical training and simulation. Although results from initial clinical experience is positive, issues of clinician acceptance, high capital costs, performance validation, and safety remain to be addressed.},
 author = {Howe, R. D. and Matsuoka, Y.},
 year = {1999},
 title = {Robotics for surgery},
 pages = {211--240},
 volume = {1},
 issn = {1523-9829},
 journal = {Annual review of biomedical engineering},
 doi = {10.1146/annurev.bioeng.1.1.211}
}


@article{Isensee.2021,
 abstract = {Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.},
 author = {Isensee, Fabian and Jaeger, Paul F. and Kohl, Simon A. A. and Petersen, Jens and Maier-Hein, Klaus H.},
 year = {2021},
 title = {nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation},
 url = {https://www.nature.com/articles/s41592-020-01008-z},
 pages = {203--211},
 volume = {18},
 number = {2},
 issn = {1548-7105},
 journal = {Nature Methods},
 doi = {10.1038/s41592-020-01008-z}
}


@article{Jaccard.1912,
 author = {Jaccard, Paul},
 year = {1912},
 title = {THE DISTRIBUTION OF THE FLORA IN THE ALPINE ZONE.1},
 pages = {37--50},
 volume = {11},
 number = {2},
 issn = {0028-646X},
 journal = {New Phytologist},
 doi = {10.1111/j.1469-8137.1912.tb05611.x},
 file = {Jaccard 1912 - THE DISTRIBUTION OF THE FLORA:Attachments/Jaccard 1912 - THE DISTRIBUTION OF THE FLORA.pdf:application/pdf}
}


@misc{Jaderberg.05Jun15,
 abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
 author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
 date = {05-Jun-15},
 title = {Spatial Transformer Networks},
 url = {https://arxiv.org/pdf/1506.02025},
 file = {Jaderberg, Simonyan et al. 05-Jun-15 - Spatial Transformer Networks:Attachments/Jaderberg, Simonyan et al. 05-Jun-15 - Spatial Transformer Networks.pdf:application/pdf}
}


@article{Jin.2022,
 abstract = {Precision radiotherapy is a critical and indispensable cancer treatment means in the modern clinical workflow with the goal of achieving ``quality-up and cost-down'' in patient care. The challenge of this therapy lies in developing computerized clinical-assistant solutions with precision, automation, and reproducibility built-in to deliver it at scale. In this work, we provide a comprehensive yet ongoing, incomplete survey of and discussions on the recent progress of utilizing advanced deep learning, semantic organ parsing, multimodal imaging fusion, neural architecture search and medical image analytical techniques to address four corner-stone problems or sub-problems required by all precision radiotherapy workflows, namely, organs at risk (OARs) segmentation, gross tumor volume (GTV) segmentation, metastasized lymph node (LN) detection, and clinical tumor volume (CTV) segmentation. Without loss of generality, we mainly focus on using esophageal and head-and-neck cancers as examples, but the methods can be extrapolated to other types of cancers. High-precision, automated and highly reproducible OAR/GTV/LN/CTV auto-delineation techniques have demonstrated their effectiveness in reducing the inter-practitioner variabilities and the time cost to permit rapid treatment planning and adaptive replanning for the benefit of patients. Through the presentation of the achievements and limitations of these techniques in this review, we hope to encourage more collective multidisciplinary precision radiotherapy workflows to transpire.},
 author = {Jin, Dakai and Guo, Dazhou and Ge, Jia and Ye, Xianghua and {Le Lu}},
 year = {2022},
 title = {Towards automated organs at risk and target volumes contouring: Defining precision radiation therapy in the modern era},
 url = {https://www.sciencedirect.com/science/article/pii/S2667005422000655},
 issn = {2667-0054},
 journal = {Journal of the National Cancer Center},
 doi = {10.1016/j.jncc.2022.09.003}
}


@inproceedings{K.He.2016,
 abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8$\times$ deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
 author = {{K. He} and {X. Zhang} and {S. Ren} and {J. Sun}},
 title = {Deep Residual Learning for Image Recognition},
 pages = {770--778},
 isbn = {1063-6919},
 booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2016},
 doi = {10.1109/CVPR.2016.90},
 file = {K. He, X. Zhang et al. 2016 - Deep Residual Learning for Image:Attachments/K. He, X. Zhang et al. 2016 - Deep Residual Learning for Image.pdf:application/pdf}
}


@article{Kaur.2022,
 abstract = {Abdominal organ segmentation is the crucial research direction in computer assisted diagnostic systems. Segmentation of multiple organs in medical images is known as Multiorgan segmentation. It is a widespread subject of research in the realm of medical image analysis. The purpose of this study is to provide the comprehensive systematic literature review on segmentation of multiple organs in abdomen CT scans. This paper focuses on the progression of state-of-art methods from traditional techniques to deep learning models. Firstly, the methods are classified into three categories: atlas based, statistical shape models and deep learning models. Secondly, research is carried out to determine which organs require more attention. The liver, kidney, and spleen are the most often selected organs, whereas the esophagus, duodenum, and portal vein are rarely picked. When medical images are taken into account for research, datasets play a vital role. This paper sheds light on publicly available datasets along with their size, no of organ classes and, related challenges which make the current study more effective and useful for the researchers in the same field. Further, evaluation metrics along with their scope and characteristics are presented. We conclude with a discussion of challenges and future directions which will open pathways for researchers. Based on the surveyed research papers, Dense-Net came out as an optimal choice. Recently, the standard practice in multi organ segmentation is two step deep learning models in sequential manner, which can take leverage of two models.},
 author = {Kaur, Harinder and Kaur, Navjot and Neeru, Nirvair},
 year = {2022},
 title = {Evolution of multiorgan segmentation techniques from traditional to deep learning in abdominal CT images -- A systematic review},
 url = {https://www.sciencedirect.com/science/article/pii/S0141938222000567},
 pages = {102223},
 volume = {73},
 issn = {0141-9382},
 journal = {Displays},
 doi = {10.1016/j.displa.2022.102223}
}


@article{KDEGroupUniversityofKassel.01Jan16,
 abstract = {The blue social bookmark and publication sharing system.},
 author = {{KDE Group, University of Kassel} and {DMIR Group, University of W{\"u}rzburg and L3S Research Center, Hannover}},
 year = {01-Jan-16},
 title = {Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (Text with EEA relevance)European Commission. (2016)},
 url = {https://www.bibsonomy.org/bibtex/c7b667cac6031282160a9e94d5a118f8},
 urldate = {30-Nov-22}
}


@article{Kiefer.1952,
 author = {Kiefer, J. and Wolfowitz, J.},
 year = {1952},
 title = {Stochastic Estimation of the Maximum of a Regression Function},
 pages = {462--466},
 volume = {23},
 number = {3},
 issn = {0003-4851},
 journal = {The Annals of Mathematical Statistics},
 doi = {10.1214/aoms/1177729392},
 file = {Kiefer, Wolfowitz 1952 - Stochastic Estimation of the Maximum:Attachments/Kiefer, Wolfowitz 1952 - Stochastic Estimation of the Maximum.pdf:application/pdf}
}


@article{Kim.2020,
 abstract = {Segmentation of normal organs is a critical and time-consuming process in radiotherapy. Auto-segmentation of abdominal organs has been made possible by the advent of the convolutional neural network. We utilized the U-Net, a 3D-patch-based convolutional neural network, and added graph-cut algorithm-based post-processing. The inputs were 3D-patch-based CT images consisting of 64 $\times$ 64 $\times$ 64 voxels designed to produce 3D multi-label semantic images representing the liver, stomach, duodenum, and right/left kidneys. The datasets for training, validating, and testing consisted of 80, 20, and 20 CT simulation scans, respectively. For accuracy assessment, the predicted structures were compared with those produced from the atlas-based method and inter-observer segmentation using the Dice similarity coefficient, Hausdorff distance, and mean surface distance. The efficiency was quantified by measuring the time elapsed for segmentation with or without automation using the U-Net. The U-Net-based auto-segmentation outperformed the atlas-based auto-segmentation in all abdominal structures, and showed comparable results to the inter-observer segmentations especially for liver and kidney. The average segmentation time without automation was 22.6 minutes, which was reduced to 7.1 minutes with automation using the U-Net. Our proposed auto-segmentation framework using the 3D-patch-based U-Net for abdominal multi-organs demonstrated potential clinical usefulness in terms of accuracy and time-efficiency.},
 author = {Kim, Hojin and Jung, Jinhong and Kim, Jieun and Cho, Byungchul and Kwak, Jungwon and Jang, Jeong Yun and Lee, Sang-wook and Lee, June-Goo and Yoon, Sang Min},
 year = {2020},
 title = {Abdominal multi-organ auto-segmentation using 3D-patch-based deep convolutional neural network},
 url = {https://www.nature.com/articles/s41598-020-63285-0},
 pages = {6204},
 volume = {10},
 number = {1},
 issn = {2045-2322},
 journal = {Scientific Reports},
 doi = {10.1038/s41598-020-63285-0},
 file = {Kim, Jung et al. 2020 - Abdominal multi-organ auto-segmentation using 3D-patch-based:Attachments/Kim, Jung et al. 2020 - Abdominal multi-organ auto-segmentation using 3D-patch-based.pdf:application/pdf;Kim, Jung et al. 2020 - Abdominal multi-organ auto-segmentation using 3D-patch-based (2):Attachments/Kim, Jung et al. 2020 - Abdominal multi-organ auto-segmentation using 3D-patch-based (2).pdf:application/pdf}
}


@incollection{Knipe.2005,
 author = {Knipe, Henry and Moore, Candace},
 title = {NIfTI (file format)},
 publisher = {Radiopaedia.org},
 booktitle = {Radiopaedia.org},
 year = {2005},
 doi = {10.53347/rID-72562}
}


@article{Lenchik.2019,
 abstract = {RATIONALE AND OBJECTIVES

The automated segmentation of organs and tissues throughout the body using computed tomography and magnetic resonance imaging has been rapidly increasing. Research into many medical conditions has benefited greatly from these approaches by allowing the development of more rapid and reproducible quantitative imaging markers. These markers have been used to help diagnose disease, determine prognosis, select patients for therapy, and follow responses to therapy. Because some of these tools are now transitioning from research environments to clinical practice, it is important for radiologists to become familiar with various methods used for automated segmentation.

MATERIALS AND METHODS

The Radiology Research Alliance of the Association of University Radiologists convened an Automated Segmentation Task Force to conduct a systematic review of the peer-reviewed literature on this topic.

RESULTS

The systematic review presented here includes 408 studies and discusses various approaches to automated segmentation using computed tomography and magnetic resonance imaging for neurologic, thoracic, abdominal, musculoskeletal, and breast imaging applications.

CONCLUSION

These insights should help prepare radiologists to better evaluate automated segmentation tools and apply them not only to research, but eventually to clinical practice.},
 author = {Lenchik, Leon and Heacock, Laura and Weaver, Ashley A. and Boutin, Robert D. and Cook, Tessa S. and Itri, Jason and Filippi, Christopher G. and Gullapalli, Rao P. and Lee, James and Zagurovskaya, Marianna and Retson, Tara and Godwin, Kendra and Nicholson, Joey and Narayana, Ponnada A.},
 year = {2019},
 title = {Automated Segmentation of Tissues Using CT and MRI: A Systematic Review},
 pages = {1695--1706},
 volume = {26},
 number = {12},
 journal = {Academic radiology},
 doi = {10.1016/j.acra.2019.07.006}
}


@article{Liu.2020,
 abstract = {PURPOSE

Segmentation of organs-at-risk (OARs) is a weak link in radiotherapeutic treatment planning process because the manual contouring action is labor-intensive and time-consuming. This work aimed to develop a deep learning-based method for rapid and accurate pancreatic multi-organ segmentation that can expedite the treatment planning process.

METHODS

We retrospectively investigated one hundred patients with computed tomography (CT) simulation scanned and contours delineated. Eight OARs including large bowel, small bowel, duodenum, left kidney, right kidney, liver, spinal cord and stomach were the target organs to be segmented. The proposed three-dimensional (3D) deep attention U-Net is featured with a deep attention strategy to effectively differentiate multiple organs. Performance of the proposed method was evaluated using six metrics, including Dice similarity coefficient (DSC), sensitivity, specificity, Hausdorff distance 95{\%} (HD95), mean surface distance (MSD) and residual mean square distance (RMSD).

RESULTS

The contours generated by the proposed method closely resemble the ground-truth manual contours, as evidenced by encouraging quantitative results in terms of DSC, sensitivity, specificity, HD95, MSD and RMSD. For DSC, mean values of 0.91~$\pm$~0.03, 0.89~$\pm$~0.06, 0.86~$\pm$~0.06, 0.95~$\pm$~0.02, 0.95~$\pm$~0.02, 0.96~$\pm$~0.01, 0.87~$\pm$~0.05 and 0.93~$\pm$~0.03 were achieved for large bowel, small bowel, duodenum, left kidney, right kidney, liver, spinal cord and stomach, respectively.

CONCLUSIONS

The proposed method could significantly expedite the treatment planning process by rapidly segmenting multiple OARs. The method could potentially be used in pancreatic adaptive radiotherapy to increase dose delivery accuracy and minimize gastrointestinal toxicity.},
 author = {Liu, Yingzi and Lei, Yang and Fu, Yabo and Wang, Tonghe and Tang, Xiangyang and Jiang, Xiaojun and Curran, Walter J. and Liu, Tian and Patel, Pretesh and Yang, Xiaofeng},
 year = {2020},
 title = {CT-based multi-organ segmentation using a 3D self-attention U-net network for pancreatic radiotherapy},
 pages = {4316--4324},
 volume = {47},
 number = {9},
 journal = {Medical physics},
 doi = {10.1002/mp.14386}
}


@article{Nai.2021,
 abstract = {Nine previously proposed segmentation evaluation metrics, targeting medical relevance, accounting for holes, and added regions or differentiating over- and under-segmentation, were compared with 24 traditional metrics to identify those which better capture the requirements for clinical segmentation evaluation. Evaluation was first performed using 2D synthetic shapes to highlight features and pitfalls of the metrics with known ground truths (GTs) and machine segmentations (MSs). Clinical evaluation was then performed using publicly-available prostate images of 20 subjects with MSs generated by 3 different deep learning networks (DenseVNet, HighRes3DNet, and ScaleNet) and GTs drawn by 2 readers. The same readers also performed the 2D visual assessment of the MSs using a dual negative-positive grading of -5 to 5 to reflect over- and under-estimation. Nine metrics that correlated well with visual assessment were selected for further evaluation using 3 different network ranking methods - based on a single metric, normalizing the metric using 2~GTs, and ranking the network based on a metric then averaging, including leave-one-out evaluation. These metrics yielded consistent ranking with HighRes3DNet ranked first then DenseVNet and ScaleNet using all ranking methods. Relative volume difference yielded the best positivity-agreement and correlation with dual visual assessment, and thus is better for providing over- and under-estimation. Interclass Correlation yielded the strongest correlation with the absolute visual assessment (0-5). Symmetric-boundary dice consistently yielded good discrimination of the networks for all three ranking methods with relatively small variations within network. Good rank discrimination may be an additional metric feature required for better network performance evaluation.},
 author = {Nai, Ying-Hwey and Teo, Bernice W. and Tan, Nadya L. and O'Doherty, Sophie and Stephenson, Mary C. and Thian, Yee Liang and Chiong, Edmund and Reilhac, Anthonin},
 year = {2021},
 title = {Comparison of metrics for the evaluation of medical segmentations using prostate MRI dataset},
 pages = {104497},
 volume = {134},
 journal = {Computers in biology and medicine},
 doi = {10.1016/j.compbiomed.2021.104497}
}


@article{Nazari.2021,
 abstract = {PURPOSE

In this work, we address image segmentation in the scope of dosimetry using deep learning and make three main contributions: (a) to extend and optimize the architecture of an existing convolutional neural network (CNN) in order to obtain a fast, robust and accurate computed tomography (CT)-based organ segmentation method for kidneys and livers; (b) to train the CNN with an inhomogeneous set of CT scans and validate the CNN for daily dosimetry; and (c) to evaluate dosimetry results obtained using automated organ segmentation in comparison with manual segmentation done by two independent experts.

METHODS

We adapted a performant deep learning approach using CT-images to delineate organ boundaries with sufficiently high accuracy and adequate processing time. The segmented organs were consequently used as binary masks for further convolution with a point spread function to retrieve the activity values from quantitatively reconstructed SPECT images for {\textquotedbl}volumetric{\textquotedbl}/3D dosimetry. The resulting activities were used to perform dosimetry calculations with the kidneys as source organs.

RESULTS

The computational expense of the algorithm was sufficient for clinical daily routine, required minimum pre-processing and performed with acceptable accuracy a Dice coefficient of [Formula: see text] for liver segmentation and of [Formula: see text] for kidney segmentation, respectively. In addition, kidney self-absorbed doses calculated using automated segmentation differed by [Formula: see text] from dosimetry performed by two medical physicists in 8 patients.

CONCLUSION

The proposed approach may accelerate volumetric dosimetry of kidneys in molecular radiotherapy with 177Lu-labelled radiopharmaceuticals such as 177Lu-DOTATOC. However, even though a fully automated segmentation methodology based on CT images accelerates organ segmentation and performs with high accuracy, it does not remove the need for supervision and corrections by experts, mostly due to misalignments in the co-registration between SPECT and CT images. Trial registration EudraCT, 2016-001897-13. Registered 26.04.2016, www.clinicaltrialsregister.eu/ctr-search/search?query=2016-001897-13 .},
 author = {Nazari, Mahmood and Jim{\'e}nez-Franco, Luis David and Schroeder, Michael and Kluge, Andreas and Bronzel, Marcus and Kimiaei, Sharok},
 year = {2021},
 title = {Automated and robust organ segmentation for 3D-based internal dose calculation},
 pages = {53},
 volume = {11},
 number = {1},
 issn = {2191-219X},
 journal = {EJNMMI research},
 doi = {10.1186/s13550-021-00796-5}
}


@inproceedings{OvergaardLauersen.2021,
 author = {{Overgaard Lauersen}, Mathilde and Koylu, Busra and Haddock, Bryan and Sorensen, John Aa.},
 title = {Kidney segmentation for quantitative analysis applying MaskRCNN architecture},
 pages = {1--6},
 publisher = {IEEE},
 isbn = {978-1-7281-9048-8},
 booktitle = {2021 IEEE Symposium Series on Computational Intelligence (SSCI)},
 year = {2021},
 address = {Piscataway, New Jersey},
 doi = {10.1109/SSCI50451.2021.9660052},
 file = {Overgaard Lauersen, Koylu et al. 12 5 2021 - 12 7 2021 - Kidney segmentation for quantitative analysis:Attachments/Overgaard Lauersen, Koylu et al. 12 5 2021 - 12 7 2021 - Kidney segmentation for quantitative analysis.pdf:application/pdf}
}


@article{Persson.2012,
 abstract = {OBJECTIVES

In radiotherapy, delineation uncertainties are important as they contribute to systematic errors and can lead to geographical miss of the target. For margin computation, standard deviations (SDs) of all uncertainties must be included as SDs. The aim of this study was to quantify the interobserver delineation variation for stereotactic body radiotherapy (SBRT) of peripheral lung tumours using a cross-sectional study design.

METHODS

22 consecutive patients with 26 tumours were included. Positron emission tomography/CT scans were acquired for planning of SBRT. Three oncologists and three radiologists independently delineated the gross tumour volume. The interobserver variation was calculated as a mean of multiple SDs of distances to a reference contour, and calculated for the transversal plane (SD(trans)) and craniocaudal (CC) direction (SD(cc)) separately. Concordance indexes and volume deviations were also calculated.

RESULTS

Median tumour volume was 13.0 cm(3), ranging from 0.3 to 60.4 cm(3). The mean SD(trans) was 0.15 cm (SD 0.08 cm) and the overall mean SD(cc) was 0.26 cm (SD 0.15 cm). Tumours with pleural contact had a significantly larger SD(trans) than tumours surrounded by lung tissue.

CONCLUSIONS

The interobserver delineation variation was very small in this systematic cross-sectional analysis, although significantly larger in the CC direction than in the transversal plane, stressing that anisotropic margins should be applied. This study is the first to make a systematic cross-sectional analysis of delineation variation for peripheral lung tumours referred for SBRT, establishing the evidence that interobserver variation is very small for these tumours.},
 author = {Persson, G. F. and Nygaard, D. E. and Hollensen, C. and {Munck af Rosensch{\"o}ld}, P. and Mouritsen, L. S. and Due, A. K. and Berthelsen, A. K. and Nyman, J. and Markova, E. and Roed, A. P. and Roed, H. and Korreman, S. and Specht, L.},
 year = {2012},
 title = {Interobserver delineation variation in lung tumour stereotactic body radiotherapy},
 pages = {e654-60},
 volume = {85},
 number = {1017},
 journal = {The British journal of radiology},
 doi = {10.1259/bjr/76424694}
}


@misc{Ren.04Jun15,
 abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
 author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
 date = {04-Jun-15},
 title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal  Networks},
 url = {https://arxiv.org/pdf/1506.01497},
 file = {Ren, He et al. 04-Jun-15 - Faster R-CNN:Attachments/Ren, He et al. 04-Jun-15 - Faster R-CNN.pdf:application/pdf}
}


@article{Sasaki.2007,
 author = {Sasaki, Yutaka},
 year = {2007},
 title = {The truth of the F-measure},
 journal = {Teach Tutor Mater}
}


@article{Schoppe.2020,
 abstract = {Whole-body imaging of mice is a key source of information for research. Organ segmentation is a prerequisite for quantitative analysis but is a tedious and error-prone task if done manually. Here, we present a deep learning solution called AIMOS that automatically segments major organs (brain, lungs, heart, liver, kidneys, spleen, bladder, stomach, intestine) and the skeleton in less than a second, orders of magnitude faster than prior algorithms. AIMOS matches or exceeds the segmentation quality of state-of-the-art approaches and of human experts. We exemplify direct applicability for biomedical research for localizing cancer metastases. Furthermore, we show that expert annotations are subject to human error and bias. As a consequence, we show that at least two independently created annotations are needed to assess model performance. Importantly, AIMOS addresses the issue of human bias by identifying the regions where humans are most likely to disagree, and thereby localizes and quantifies this uncertainty for improved downstream analysis. In summary, AIMOS is a powerful open-source tool to increase scalability, reduce bias, and foster reproducibility in many areas of biomedical research.},
 author = {Schoppe, Oliver and Pan, Chenchen and Coronel, Javier and Mai, Hongcheng and Rong, Zhouyi and Todorov, Mihail Ivilinov and M{\"u}skes, Annemarie and Navarro, Fernando and Li, Hongwei and Ert{\"u}rk, Ali and Menze, Bjoern H.},
 year = {2020},
 title = {Deep learning-enabled multi-organ segmentation in whole-body mouse scans},
 url = {https://www.nature.com/articles/s41467-020-19449-7},
 pages = {5626},
 volume = {11},
 number = {1},
 issn = {2041-1723},
 journal = {Nature Communications},
 doi = {10.1038/s41467-020-19449-7},
 file = {Schoppe, Pan et al. 2020 - Deep learning-enabled multi-organ segmentation:Attachments/Schoppe, Pan et al. 2020 - Deep learning-enabled multi-organ segmentation.pdf:application/pdf}
}


@article{Sharma.21.08.2019,
 abstract = {The difference between Image Classification, Object Detection and Image Segmentation in the context of Computer Vision},
 author = {Sharma, Pulkit},
 year = {21.08.2019},
 title = {Image Classification vs. Object Detection vs. Image Segmentation},
 url = {https://medium.com/analytics-vidhya/image-classification-vs-object-detection-vs-image-segmentation-f36db85fe81},
 urldate = {08.05.2022},
 journal = {Analytics Vidhya}
}


@article{Shu.2020,
 author = {Shu, Jian-Hua and Nian, Fu-Dong and Yu, Ming-Hui and Li, Xu},
 year = {2020},
 title = {An Improved Mask R-CNN Model for Multiorgan Segmentation},
 pages = {1--11},
 volume = {2020},
 issn = {1024-123X},
 journal = {Mathematical Problems in Engineering},
 doi = {10.1155/2020/8351725},
 file = {Shu, Nian et al. 2020 - An Improved Mask R-CNN Model:Attachments/Shu, Nian et al. 2020 - An Improved Mask R-CNN Model.pdf:application/pdf}
}


@article{vanGinneken.2011,
 abstract = {Computer-aided diagnosis (CAD), encompassing computer-aided detection and quantification, is an established and rapidly growing field of research. In daily practice, however, most radiologists do not yet use CAD routinely. This article discusses how to move CAD from the laboratory to the clinic. The authors review the principles of CAD for lesion detection and for quantification and illustrate the state-of-the-art with various examples. The requirements that radiologists have for CAD are discussed: sufficient performance, no increase in reading time, seamless workflow integration, regulatory approval, and cost efficiency. Performance is still the major bottleneck for many CAD systems. Novel ways of using CAD, extending the traditional paradigm of displaying markers for a second look, may be the key to using the technology effectively. The most promising strategy to improve CAD is the creation of publicly available databases for training and validation. This can identify the most fruitful new research directions, and provide a platform to combine multiple approaches for a single task to create superior algorithms.},
 author = {{van Ginneken}, Bram and Schaefer-Prokop, Cornelia M. and Prokop, Mathias},
 year = {2011},
 title = {Computer-aided diagnosis: how to move from the laboratory to the clinic},
 pages = {719--732},
 volume = {261},
 number = {3},
 journal = {Radiology},
 doi = {10.1148/radiol.11091710},
 file = {van Ginneken, Schaefer-Prokop et al. 2011 - Computer-aided diagnosis:Attachments/van Ginneken, Schaefer-Prokop et al. 2011 - Computer-aided diagnosis.pdf:application/pdf}
}


@article{vanMourik.2010,
 abstract = {PURPOSE

This study aims to determine magnitude, causes and consequences of post-operative breast tumour target volume delineation variation among radiation oncologists in the presence of guidelines.

MATERIALS AND METHODS

Excision cavities, CTVs and PTVs of eight breast cancer patients were delineated on CT scans by 13 Dutch radiation oncologists (observers) from 12 Dutch institutes participating in the international Young Boost Trial. Delineated volumes and conformity indices were determined. CTV delineation variation (SD) was determined for anatomically relevant regions. Non-parametric statistics were performed to establish effects of observers, patient characteristics and regions on delineation variation.

RESULTS

Even in the presence of delineation guidelines considerable delineation variation is present (0.24{\textless}SD{\textless}1.22 cm). Presence of clips or seroma reduced interobserver variation (0.24{\textless}SD{\textless}0.62 cm). Region-specific analysis showed distinct regions of higher variability per patient. This could not always be ascribed to anatomical features, suggesting interobserver variation is not solely due to lack of image quality.

CONCLUSIONS

In this study, interobserver delineation variation in breast tumour target volume delineation is larger than, e.g. setup inaccuracies and results from limited reliable visual guidance as well as interpretation differences between observers, despite guidelines. Reduction of delineation variation is essential in view of current developments in planning techniques, particularly for External Partial Breast Irradiation.},
 author = {{van Mourik}, Anke M. and Elkhuizen, Paula H. M. and Minkema, Danny and Duppen, Joop C. and {van Vliet-Vroegindeweij}, Corine},
 year = {2010},
 title = {Multiinstitutional study on target volume delineation variation in breast radiotherapy in the presence of guidelines},
 pages = {286--291},
 volume = {94},
 number = {3},
 journal = {Radiotherapy and oncology : journal of the European Society for Therapeutic Radiology and Oncology},
 doi = {10.1016/j.radonc.2010.01.009}
}


@misc{Vuola.29Jan19,
 abstract = {Nuclei segmentation is both an important and in some ways ideal task for modern computer vision methods, e.g. convolutional neural networks. While recent developments in theory and open-source software have made these tools easier to implement, expert knowledge is still required to choose the right model architecture and training setup. We compare two popular segmentation frameworks, U-Net and Mask-RCNN in the nuclei segmentation task and find that they have different strengths and failures. To get the best of both worlds, we develop an ensemble model to combine their predictions that can outperform both models by a significant margin and should be considered when aiming for best nuclei segmentation performance.},
 author = {Vuola, Aarno Oskar and Akram, Saad Ullah and Kannala, Juho},
 date = {29-Jan-19},
 title = {Mask-RCNN and U-net Ensembled for Nuclei Segmentation},
 url = {https://arxiv.org/pdf/1901.10170},
 file = {Vuola, Akram et al. 29-Jan-19 - Mask-RCNN and U-net Ensembled:Attachments/Vuola, Akram et al. 29-Jan-19 - Mask-RCNN and U-net Ensembled.pdf:application/pdf}
}


